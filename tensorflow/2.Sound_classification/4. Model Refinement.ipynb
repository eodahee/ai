{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Refinement\n",
    "\n",
    "- 모델 강화학습(딥러닝을 사용해 urban sounds 분류하기)\n",
    "  * CNN 모델을 생성해서 딥러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞서 2번에서 전처리 완료된 데이터 저장한 것을 불러오기\n",
    "\n",
    "%store -r x_train \n",
    "%store -r x_test \n",
    "%store -r y_train \n",
    "%store -r y_test \n",
    "%store -r yy \n",
    "%store -r le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특징 추출 개선(refinement)\n",
    "# 이전에 추출한 특징(feature) MFCC 벡터는 오디오파일마다 크기 다름\n",
    "# CNN(Convolutional Neural Network) 만들기 위해서는 모두 동일한 크기로 만들어야 함(벡터를 0으로 패딩)\n",
    "import numpy as np\n",
    "import librosa\n",
    "max_pad_len = 174\n",
    "\n",
    "def extract_features(file_name):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "     \n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished feature extraction from  8732  files\n"
     ]
    }
   ],
   "source": [
    "# Load various imports \n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "# Set the path to the full UrbanSound dataset \n",
    "fulldatasetpath = './UrbanSound8K/audio'\n",
    "\n",
    "metadata = pd.read_csv('./UrbanSound8K/metadata/UrbanSound8K.csv')\n",
    "\n",
    "features = []\n",
    "\n",
    "# Iterate through each sound file and extract the features \n",
    "for index, row in metadata.iterrows():  \n",
    "    file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    \n",
    "    class_label = row[\"class_name\"]\n",
    "    data = extract_features(file_name)\n",
    "    \n",
    "    features.append([data, class_label])\n",
    "\n",
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                feature       class_label\n",
      "0     [[-306.77255, -177.59209, -99.13616, -65.97198...          dog_bark\n",
      "1     [[-457.69534, -451.0248, -450.68613, -445.0000...  children_playing\n",
      "2     [[-468.0367, -467.42264, -481.04654, -486.5948...  children_playing\n",
      "3     [[-422.42215, -411.9085, -409.46243, -409.0892...  children_playing\n",
      "4     [[-438.10162, -434.47787, -443.32837, -442.664...  children_playing\n",
      "...                                                 ...               ...\n",
      "8727  [[-397.82446, -400.45578, -407.50354, -408.952...          car_horn\n",
      "8728  [[-451.81265, -451.41983, -450.67892, -445.635...          car_horn\n",
      "8729  [[-301.06348, -298.25397, -305.0326, -303.8614...          car_horn\n",
      "8730  [[-373.6307, -369.44986, -366.48, -364.9094, -...          car_horn\n",
      "8731  [[-309.34647, -305.3132, -308.23593, -308.1856...          car_horn\n",
      "\n",
      "[8732 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(featuresdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-8.37522125e+01 -7.18939133e+01 -7.44299469e+01 ... -9.78421860e+01\n",
      "   -9.42385483e+01  0.00000000e+00]\n",
      "  [ 1.14666840e+02  1.19127029e+02  1.25082397e+02 ...  1.12450966e+02\n",
      "    1.06469528e+02  0.00000000e+00]\n",
      "  [-7.31703949e+01 -8.15858688e+01 -8.74292984e+01 ... -1.00178772e+02\n",
      "   -1.00562012e+02  0.00000000e+00]\n",
      "  ...\n",
      "  [ 6.21281624e-01 -1.46681368e-02  9.38629746e-01 ...  5.45317411e-01\n",
      "    3.08528042e+00  0.00000000e+00]\n",
      "  [ 3.85332060e+00  1.10046196e+00  1.51432705e+00 ...  4.19227362e+00\n",
      "    4.53711987e+00  0.00000000e+00]\n",
      "  [ 3.23432970e+00 -2.41597605e+00 -1.52170730e+00 ... -1.35237694e+01\n",
      "   -1.04434299e+01  0.00000000e+00]]\n",
      "\n",
      " [[-2.08063232e+02 -2.07646057e+02 -2.09871704e+02 ... -1.68888702e+02\n",
      "   -1.64623489e+02  0.00000000e+00]\n",
      "  [ 1.33095703e+02  1.34623871e+02  1.34608337e+02 ...  1.33464355e+02\n",
      "    1.22665115e+02  0.00000000e+00]\n",
      "  [-1.22376442e+01 -2.17055168e+01 -2.98256149e+01 ... -2.23849869e+01\n",
      "   -3.11693134e+01  0.00000000e+00]\n",
      "  ...\n",
      "  [-6.87027740e+00 -6.54786634e+00 -4.23049164e+00 ... -4.43638992e+00\n",
      "   -4.77812004e+00  0.00000000e+00]\n",
      "  [ 3.46669108e-01 -6.83280110e-01  2.96805352e-01 ... -3.45153093e+00\n",
      "    2.08491850e+00  0.00000000e+00]\n",
      "  [ 2.35520196e+00 -1.92890120e+00 -3.52806282e+00 ...  4.61266565e+00\n",
      "    5.27835941e+00  0.00000000e+00]]\n",
      "\n",
      " [[-1.89538101e+02 -2.28362381e+02 -2.87981476e+02 ... -2.85434265e+02\n",
      "   -2.17932663e+02  0.00000000e+00]\n",
      "  [ 1.73684113e+02  1.76574738e+02  1.74323990e+02 ...  1.74996307e+02\n",
      "    1.68870941e+02  0.00000000e+00]\n",
      "  [-8.95009003e+01 -1.00933243e+02 -9.45188446e+01 ... -7.95534363e+01\n",
      "   -5.79045639e+01  0.00000000e+00]\n",
      "  ...\n",
      "  [-4.48744327e-01 -2.88910055e+00 -1.77972424e+00 ...  9.28261280e-02\n",
      "    1.90458059e+00  0.00000000e+00]\n",
      "  [ 1.03974366e+00  7.34797239e-01  3.52628875e+00 ... -1.07646475e+01\n",
      "   -9.65968800e+00  0.00000000e+00]\n",
      "  [ 1.55074239e+00  2.69934154e+00 -1.10802412e+00 ... -2.56183958e+00\n",
      "   -3.69100380e+00  0.00000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.74254059e+02 -1.97625275e+02 -2.51811295e+02 ... -3.00661041e+02\n",
      "   -2.81804260e+02  0.00000000e+00]\n",
      "  [ 2.07248260e+02  2.23776642e+02  2.44256897e+02 ...  2.26118103e+02\n",
      "    2.30651794e+02  0.00000000e+00]\n",
      "  [ 1.44205837e+01  2.92463541e+00 -1.72033272e+01 ... -5.59539032e+00\n",
      "   -8.09025002e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 7.84351230e-01 -3.58631134e-01 -5.65928102e-01 ...  2.95648432e+00\n",
      "    3.25867414e+00  0.00000000e+00]\n",
      "  [-4.05253792e+00 -3.10810781e+00 -6.02176666e-01 ...  8.08596849e-01\n",
      "    5.08444071e-01  0.00000000e+00]\n",
      "  [-9.65382159e-01 -5.99062204e-01 -2.91872382e-01 ...  3.76111269e-02\n",
      "    2.95355892e+00  0.00000000e+00]]\n",
      "\n",
      " [[-2.48060898e+02 -2.54687607e+02 -2.59105225e+02 ... -1.81723068e+02\n",
      "   -1.73559189e+02  0.00000000e+00]\n",
      "  [ 7.29875183e+01  8.36813965e+01  9.53141251e+01 ...  9.35885468e+01\n",
      "    1.07010818e+02  0.00000000e+00]\n",
      "  [-6.60560455e+01 -6.78139267e+01 -7.27602844e+01 ... -8.55730438e+01\n",
      "   -7.65078583e+01  0.00000000e+00]\n",
      "  ...\n",
      "  [ 9.54567432e+00  8.08279896e+00  9.14508820e+00 ...  1.61216545e+00\n",
      "   -1.27549839e+00  0.00000000e+00]\n",
      "  [-1.82327819e+00 -4.16958523e+00 -1.37168193e+00 ... -5.01886082e+00\n",
      "   -9.83475494e+00  0.00000000e+00]\n",
      "  [-5.77557659e+00 -2.03513646e+00  3.26921606e+00 ... -3.57975006e+00\n",
      "   -4.99022293e+00  0.00000000e+00]]\n",
      "\n",
      " [[-2.87441559e+02 -2.78800751e+02 -2.78439636e+02 ... -2.38245132e+02\n",
      "   -2.51177933e+02  0.00000000e+00]\n",
      "  [ 1.24562607e+02  1.39636337e+02  1.48228699e+02 ...  1.00919235e+02\n",
      "    9.98964157e+01  0.00000000e+00]\n",
      "  [-2.65903664e+01 -2.78581562e+01 -3.16512299e+01 ...  1.52470875e+00\n",
      "    1.77112472e+00  0.00000000e+00]\n",
      "  ...\n",
      "  [ 1.14278090e+00  2.12472975e-01 -4.25775498e-01 ...  3.21381330e+00\n",
      "   -2.18321061e+00  0.00000000e+00]\n",
      "  [ 2.90975952e+00  2.53101015e+00 -1.30724812e+00 ... -3.78011703e-01\n",
      "   -4.31407881e+00  0.00000000e+00]\n",
      "  [ 4.31149101e+00  7.81975508e-01 -3.38790417e+00 ...  1.80462909e+00\n",
      "   -6.18091202e+00  0.00000000e+00]]]\n",
      "(6985, 40, 174)\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN(Convolutional Neural Network) 모델 구조로 수정(재생성)\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "num_rows = 40\n",
    "num_columns = 174\n",
    "num_channels = 1\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일하기(이전 모델과 동일한 옵션)\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 39, 173, 16)       80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 18, 85, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 9, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 9, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 41, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 3, 19, 128)        32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 1, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 44,602\n",
      "Trainable params: 44,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1747/1747 [==============================] - 4s 2ms/step\n",
      "Pre-training accuracy: 13.2227%\n"
     ]
    }
   ],
   "source": [
    "model.summary() # 모델 보여주기\n",
    "\n",
    "# pre-training 학습 정확도 계산\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6985 samples, validate on 1747 samples\n",
      "Epoch 1/72\n",
      "6985/6985 [==============================] - 62s 9ms/step - loss: 5.3670 - accuracy: 0.1529 - val_loss: 2.2611 - val_accuracy: 0.1763\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.26111, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 2/72\n",
      "6985/6985 [==============================] - 58s 8ms/step - loss: 2.0962 - accuracy: 0.2610 - val_loss: 1.9411 - val_accuracy: 0.3612\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.26111 to 1.94114, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 3/72\n",
      "6985/6985 [==============================] - 60s 9ms/step - loss: 1.7015 - accuracy: 0.3996 - val_loss: 1.6538 - val_accuracy: 0.4385\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.94114 to 1.65379, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 4/72\n",
      "6985/6985 [==============================] - 63s 9ms/step - loss: 1.5221 - accuracy: 0.4630 - val_loss: 1.5353 - val_accuracy: 0.5197\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.65379 to 1.53527, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 5/72\n",
      "6985/6985 [==============================] - 61s 9ms/step - loss: 1.4188 - accuracy: 0.4988 - val_loss: 1.4813 - val_accuracy: 0.5266\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.53527 to 1.48129, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 6/72\n",
      "6985/6985 [==============================] - 59s 8ms/step - loss: 1.3450 - accuracy: 0.5290 - val_loss: 1.4142 - val_accuracy: 0.5438\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.48129 to 1.41424, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 7/72\n",
      "6985/6985 [==============================] - 61s 9ms/step - loss: 1.2709 - accuracy: 0.5565 - val_loss: 1.3468 - val_accuracy: 0.5633\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.41424 to 1.34677, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 8/72\n",
      "6985/6985 [==============================] - 64s 9ms/step - loss: 1.2080 - accuracy: 0.5788 - val_loss: 1.2728 - val_accuracy: 0.5982\n",
      "\n",
      "Epoch 00008: val_loss improved from 1.34677 to 1.27280, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 9/72\n",
      "6985/6985 [==============================] - 62s 9ms/step - loss: 1.1538 - accuracy: 0.5957 - val_loss: 1.2107 - val_accuracy: 0.6182\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.27280 to 1.21074, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 10/72\n",
      "6985/6985 [==============================] - 62s 9ms/step - loss: 1.1109 - accuracy: 0.6137 - val_loss: 1.2088 - val_accuracy: 0.6102\n",
      "\n",
      "Epoch 00010: val_loss improved from 1.21074 to 1.20877, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 11/72\n",
      "6985/6985 [==============================] - 60s 9ms/step - loss: 1.0674 - accuracy: 0.6378 - val_loss: 1.1100 - val_accuracy: 0.6503\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.20877 to 1.11003, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 12/72\n",
      "6985/6985 [==============================] - 58s 8ms/step - loss: 1.0321 - accuracy: 0.6438 - val_loss: 1.0808 - val_accuracy: 0.6560\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.11003 to 1.08076, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 13/72\n",
      "6985/6985 [==============================] - 58s 8ms/step - loss: 0.9975 - accuracy: 0.6495 - val_loss: 1.0567 - val_accuracy: 0.6600\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.08076 to 1.05675, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 14/72\n",
      "6985/6985 [==============================] - 58s 8ms/step - loss: 0.9597 - accuracy: 0.6713 - val_loss: 1.0305 - val_accuracy: 0.6800\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.05675 to 1.03051, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 15/72\n",
      "6985/6985 [==============================] - 58s 8ms/step - loss: 0.9128 - accuracy: 0.6853 - val_loss: 0.9849 - val_accuracy: 0.6835\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.03051 to 0.98488, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 16/72\n",
      "6985/6985 [==============================] - 59s 8ms/step - loss: 0.8971 - accuracy: 0.6952 - val_loss: 0.9166 - val_accuracy: 0.7224\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.98488 to 0.91658, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 17/72\n",
      "6985/6985 [==============================] - 59s 8ms/step - loss: 0.8449 - accuracy: 0.7144 - val_loss: 0.8893 - val_accuracy: 0.7327\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.91658 to 0.88931, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 18/72\n",
      "6985/6985 [==============================] - 64s 9ms/step - loss: 0.8291 - accuracy: 0.7195 - val_loss: 0.8790 - val_accuracy: 0.7287\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.88931 to 0.87896, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 19/72\n",
      "6985/6985 [==============================] - 58s 8ms/step - loss: 0.8117 - accuracy: 0.7194 - val_loss: 0.8416 - val_accuracy: 0.7487\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.87896 to 0.84160, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 20/72\n",
      "6985/6985 [==============================] - 60s 9ms/step - loss: 0.7900 - accuracy: 0.7288 - val_loss: 0.8532 - val_accuracy: 0.7293\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.84160\n",
      "Epoch 21/72\n",
      "6985/6985 [==============================] - 61s 9ms/step - loss: 0.7647 - accuracy: 0.7414 - val_loss: 0.7949 - val_accuracy: 0.7504\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.84160 to 0.79493, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 22/72\n",
      "6985/6985 [==============================] - 60s 9ms/step - loss: 0.7460 - accuracy: 0.7485 - val_loss: 0.7950 - val_accuracy: 0.7499\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.79493\n",
      "Epoch 23/72\n",
      "6985/6985 [==============================] - 59s 8ms/step - loss: 0.7271 - accuracy: 0.7505 - val_loss: 0.7622 - val_accuracy: 0.7533\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.79493 to 0.76219, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 24/72\n",
      "6985/6985 [==============================] - 64s 9ms/step - loss: 0.7137 - accuracy: 0.7599 - val_loss: 0.7540 - val_accuracy: 0.7642\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.76219 to 0.75401, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 25/72\n",
      "6985/6985 [==============================] - 68s 10ms/step - loss: 0.6769 - accuracy: 0.7699 - val_loss: 0.7232 - val_accuracy: 0.7808\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.75401 to 0.72319, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 26/72\n",
      "6985/6985 [==============================] - 58s 8ms/step - loss: 0.6798 - accuracy: 0.7741 - val_loss: 0.7231 - val_accuracy: 0.7745\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.72319 to 0.72310, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 27/72\n",
      "6985/6985 [==============================] - 62s 9ms/step - loss: 0.6484 - accuracy: 0.7828 - val_loss: 0.6869 - val_accuracy: 0.7974\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.72310 to 0.68686, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 28/72\n",
      "6985/6985 [==============================] - 61s 9ms/step - loss: 0.6390 - accuracy: 0.7811 - val_loss: 0.6964 - val_accuracy: 0.7808\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.68686\n",
      "Epoch 29/72\n",
      "6985/6985 [==============================] - 60s 9ms/step - loss: 0.6309 - accuracy: 0.7931 - val_loss: 0.6528 - val_accuracy: 0.7991\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.68686 to 0.65284, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 30/72\n",
      "6985/6985 [==============================] - 79s 11ms/step - loss: 0.6046 - accuracy: 0.7971 - val_loss: 0.6315 - val_accuracy: 0.8226\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.65284 to 0.63152, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 31/72\n",
      "6985/6985 [==============================] - 62s 9ms/step - loss: 0.6001 - accuracy: 0.8003 - val_loss: 0.6089 - val_accuracy: 0.8163\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.63152 to 0.60888, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 32/72\n",
      "6985/6985 [==============================] - 59s 8ms/step - loss: 0.5981 - accuracy: 0.8007 - val_loss: 0.6354 - val_accuracy: 0.8025\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.60888\n",
      "Epoch 33/72\n",
      "6985/6985 [==============================] - 60s 9ms/step - loss: 0.5747 - accuracy: 0.8050 - val_loss: 0.5918 - val_accuracy: 0.8220\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.60888 to 0.59179, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 34/72\n",
      "6985/6985 [==============================] - 60s 9ms/step - loss: 0.5679 - accuracy: 0.8129 - val_loss: 0.5925 - val_accuracy: 0.8185\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.59179\n",
      "Epoch 35/72\n",
      "6985/6985 [==============================] - 62s 9ms/step - loss: 0.5519 - accuracy: 0.8147 - val_loss: 0.5703 - val_accuracy: 0.8277\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.59179 to 0.57030, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 36/72\n",
      "6985/6985 [==============================] - 61s 9ms/step - loss: 0.5312 - accuracy: 0.8146 - val_loss: 0.5764 - val_accuracy: 0.8260\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.57030\n",
      "Epoch 37/72\n",
      "6985/6985 [==============================] - 62s 9ms/step - loss: 0.5356 - accuracy: 0.8220 - val_loss: 0.5375 - val_accuracy: 0.8449\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.57030 to 0.53747, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 38/72\n",
      "6985/6985 [==============================] - 60s 9ms/step - loss: 0.5232 - accuracy: 0.8208 - val_loss: 0.5334 - val_accuracy: 0.8397\n",
      "\n",
      "Epoch 00038: val_loss improved from 0.53747 to 0.53337, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 39/72\n",
      "6985/6985 [==============================] - 60s 9ms/step - loss: 0.4938 - accuracy: 0.8344 - val_loss: 0.5312 - val_accuracy: 0.8392\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.53337 to 0.53117, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 40/72\n",
      "6985/6985 [==============================] - 59s 8ms/step - loss: 0.4811 - accuracy: 0.8348 - val_loss: 0.5471 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.53117\n",
      "Epoch 41/72\n",
      "6985/6985 [==============================] - 61s 9ms/step - loss: 0.4935 - accuracy: 0.8328 - val_loss: 0.5348 - val_accuracy: 0.8300\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.53117\n",
      "Epoch 42/72\n",
      "6985/6985 [==============================] - 63s 9ms/step - loss: 0.4888 - accuracy: 0.8315 - val_loss: 0.5415 - val_accuracy: 0.8248\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.53117\n",
      "Epoch 43/72\n",
      "6985/6985 [==============================] - 69s 10ms/step - loss: 0.4857 - accuracy: 0.8362 - val_loss: 0.5127 - val_accuracy: 0.8460\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.53117 to 0.51270, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 44/72\n",
      "6985/6985 [==============================] - 63s 9ms/step - loss: 0.4622 - accuracy: 0.8475 - val_loss: 0.4927 - val_accuracy: 0.8449\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.51270 to 0.49272, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 45/72\n",
      "6985/6985 [==============================] - 64s 9ms/step - loss: 0.4568 - accuracy: 0.8444 - val_loss: 0.4831 - val_accuracy: 0.8443\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.49272 to 0.48311, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 46/72\n",
      "6985/6985 [==============================] - 62s 9ms/step - loss: 0.4299 - accuracy: 0.8554 - val_loss: 0.4573 - val_accuracy: 0.8620\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.48311 to 0.45733, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 47/72\n",
      "6985/6985 [==============================] - 59s 8ms/step - loss: 0.4381 - accuracy: 0.8490 - val_loss: 0.4580 - val_accuracy: 0.8672\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.45733\n",
      "Epoch 48/72\n",
      "6985/6985 [==============================] - 66s 9ms/step - loss: 0.4253 - accuracy: 0.8520 - val_loss: 0.4971 - val_accuracy: 0.8460\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.45733\n",
      "Epoch 49/72\n",
      "6985/6985 [==============================] - 69s 10ms/step - loss: 0.4203 - accuracy: 0.8626 - val_loss: 0.4610 - val_accuracy: 0.8592\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.45733\n",
      "Epoch 50/72\n",
      "6985/6985 [==============================] - 67s 10ms/step - loss: 0.4238 - accuracy: 0.8548 - val_loss: 0.4667 - val_accuracy: 0.8620\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.45733\n",
      "Epoch 51/72\n",
      "6985/6985 [==============================] - 60s 9ms/step - loss: 0.4060 - accuracy: 0.8624 - val_loss: 0.4337 - val_accuracy: 0.8758\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.45733 to 0.43373, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 52/72\n",
      "6985/6985 [==============================] - 62s 9ms/step - loss: 0.3969 - accuracy: 0.8617 - val_loss: 0.4423 - val_accuracy: 0.8678\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.43373\n",
      "Epoch 53/72\n",
      "6985/6985 [==============================] - 58s 8ms/step - loss: 0.3820 - accuracy: 0.8703 - val_loss: 0.4444 - val_accuracy: 0.8666\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.43373\n",
      "Epoch 54/72\n",
      "6985/6985 [==============================] - 59s 9ms/step - loss: 0.3741 - accuracy: 0.8749 - val_loss: 0.4396 - val_accuracy: 0.8661\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.43373\n",
      "Epoch 55/72\n",
      "6985/6985 [==============================] - 60s 9ms/step - loss: 0.3658 - accuracy: 0.8739 - val_loss: 0.4276 - val_accuracy: 0.8672\n",
      "\n",
      "Epoch 00055: val_loss improved from 0.43373 to 0.42763, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 56/72\n",
      "6985/6985 [==============================] - 60s 9ms/step - loss: 0.3594 - accuracy: 0.8753 - val_loss: 0.3994 - val_accuracy: 0.8798\n",
      "\n",
      "Epoch 00056: val_loss improved from 0.42763 to 0.39938, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 57/72\n",
      "6985/6985 [==============================] - 59s 8ms/step - loss: 0.3785 - accuracy: 0.8677 - val_loss: 0.4014 - val_accuracy: 0.8764\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.39938\n",
      "Epoch 58/72\n",
      "6985/6985 [==============================] - 60s 9ms/step - loss: 0.3739 - accuracy: 0.8690 - val_loss: 0.4170 - val_accuracy: 0.8764\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.39938\n",
      "Epoch 59/72\n",
      "6985/6985 [==============================] - 61s 9ms/step - loss: 0.3440 - accuracy: 0.8810 - val_loss: 0.3941 - val_accuracy: 0.8804\n",
      "\n",
      "Epoch 00059: val_loss improved from 0.39938 to 0.39408, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 60/72\n",
      "6985/6985 [==============================] - 62s 9ms/step - loss: 0.3517 - accuracy: 0.8779 - val_loss: 0.4323 - val_accuracy: 0.8683\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.39408\n",
      "Epoch 61/72\n",
      "6985/6985 [==============================] - 61s 9ms/step - loss: 0.3390 - accuracy: 0.8853 - val_loss: 0.3967 - val_accuracy: 0.8844\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.39408\n",
      "Epoch 62/72\n",
      "6985/6985 [==============================] - 61s 9ms/step - loss: 0.3273 - accuracy: 0.8849 - val_loss: 0.3746 - val_accuracy: 0.8786\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.39408 to 0.37456, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 63/72\n",
      "6985/6985 [==============================] - 60s 9ms/step - loss: 0.3348 - accuracy: 0.8880 - val_loss: 0.3848 - val_accuracy: 0.8741\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.37456\n",
      "Epoch 64/72\n",
      "6985/6985 [==============================] - 61s 9ms/step - loss: 0.3244 - accuracy: 0.8903 - val_loss: 0.4110 - val_accuracy: 0.8683\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.37456\n",
      "Epoch 65/72\n",
      "6985/6985 [==============================] - 65s 9ms/step - loss: 0.3172 - accuracy: 0.8866 - val_loss: 0.3855 - val_accuracy: 0.8815\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.37456\n",
      "Epoch 66/72\n",
      "6985/6985 [==============================] - 62s 9ms/step - loss: 0.3294 - accuracy: 0.8827 - val_loss: 0.3714 - val_accuracy: 0.8832\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.37456 to 0.37137, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 67/72\n",
      "6985/6985 [==============================] - 61s 9ms/step - loss: 0.3061 - accuracy: 0.8958 - val_loss: 0.3610 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.37137 to 0.36098, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 68/72\n",
      "6985/6985 [==============================] - 61s 9ms/step - loss: 0.3197 - accuracy: 0.8909 - val_loss: 0.3870 - val_accuracy: 0.8809\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.36098\n",
      "Epoch 69/72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6985/6985 [==============================] - 63s 9ms/step - loss: 0.2960 - accuracy: 0.8986 - val_loss: 0.3777 - val_accuracy: 0.8861\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.36098\n",
      "Epoch 70/72\n",
      "6985/6985 [==============================] - 58s 8ms/step - loss: 0.2810 - accuracy: 0.9034 - val_loss: 0.3602 - val_accuracy: 0.8907\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.36098 to 0.36019, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 71/72\n",
      "6985/6985 [==============================] - 62s 9ms/step - loss: 0.2723 - accuracy: 0.9094 - val_loss: 0.3465 - val_accuracy: 0.8970\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.36019 to 0.34649, saving model to saved_models/weights.best.basic_cnn.hdf5\n",
      "Epoch 72/72\n",
      "6985/6985 [==============================] - 62s 9ms/step - loss: 0.3007 - accuracy: 0.8989 - val_loss: 0.3658 - val_accuracy: 0.8924\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.34649\n",
      "Training completed in time:  1:13:47.073305\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# 해당 모델에 대해 epoch 72회 실시(72번 반복) \n",
    "# CNN 모델의 특성상 이전 basic 모델과 다르게 시간이 굉장히 오래 걸림\n",
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "#num_epochs = 12\n",
    "#num_batch_size = 128\n",
    "\n",
    "num_epochs = 72\n",
    "num_batch_size = 256\n",
    "\n",
    "# 학습(Training)하는 동안 체크포인트 저장하기\n",
    "# 훈련 중간과 마지막에 자동으로 저장하도록 옵션 설정(모델 재사용성 up!)\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "# 모델 학습(Training)\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9367215633392334\n",
      "Testing Accuracy:  0.8923869729042053\n"
     ]
    }
   ],
   "source": [
    "# 모델 테스트하기\n",
    "# Train dataset\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "# Test dataset\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6985, 40, 174, 1)\n",
      "(6985, 10)\n",
      "(1747, 40, 174, 1)\n",
      "(1747, 10)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측하기(함수 생성)\n",
    "def print_prediction(file_name):\n",
    "    prediction_feature = extract_features(file_name) \n",
    "    prediction_feature = prediction_feature.reshape(1, num_rows, num_columns, num_channels)\n",
    "\n",
    "    predicted_vector = model.predict_classes(prediction_feature)\n",
    "    predicted_class = le.inverse_transform(predicted_vector) \n",
    "    print(\"The predicted class is:\", predicted_class[0], '\\n') \n",
    "\n",
    "    predicted_proba_vector = model.predict_proba(prediction_feature) \n",
    "    predicted_proba = predicted_proba_vector[0]\n",
    "    for i in range(len(predicted_proba)): \n",
    "        category = le.inverse_transform(np.array([i]))\n",
    "        print(category[0], \"\\t\\t : \", format(predicted_proba[i], '.32f') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: air_conditioner \n",
      "\n",
      "air_conditioner \t\t :  0.95868599414825439453125000000000\n",
      "car_horn \t\t :  0.00002513961590011604130268096924\n",
      "children_playing \t\t :  0.02091950178146362304687500000000\n",
      "dog_bark \t\t :  0.00067896617110818624496459960938\n",
      "drilling \t\t :  0.00735024036839604377746582031250\n",
      "engine_idling \t\t :  0.00765555538237094879150390625000\n",
      "gun_shot \t\t :  0.00015567925584036856889724731445\n",
      "jackhammer \t\t :  0.00091366545530036091804504394531\n",
      "siren \t\t :  0.00042527535697445273399353027344\n",
      "street_music \t\t :  0.00318997912108898162841796875000\n"
     ]
    }
   ],
   "source": [
    "# Class: Air Conditioner\n",
    "\n",
    "filename = './UrbanSound8K/audio/fold5/100852-0-0-0.wav' \n",
    "print_prediction(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: siren \n",
      "\n",
      "air_conditioner \t\t :  0.00000000000000000014355622595630\n",
      "car_horn \t\t :  0.00000000000000003857009854820024\n",
      "children_playing \t\t :  0.00000000040563058467668611228873\n",
      "dog_bark \t\t :  0.00000067011859528065542690455914\n",
      "drilling \t\t :  0.00000000001219694657772496526604\n",
      "engine_idling \t\t :  0.00000000031732638738901641772827\n",
      "gun_shot \t\t :  0.00000000009801592176783557874842\n",
      "jackhammer \t\t :  0.00000000000001572945095842329805\n",
      "siren \t\t :  0.99999773502349853515625000000000\n",
      "street_music \t\t :  0.00000151486824506719131022691727\n"
     ]
    }
   ],
   "source": [
    "# Class: Siren\n",
    "\n",
    "filename = './UrbanSound8K/audio/fold9/159748-8-2-1.wav' \n",
    "print_prediction(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error encountered while parsing file:  ./children.wav\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-b36188c8afc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./children.wav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-0f9caeb8d96d>\u001b[0m in \u001b[0;36mprint_prediction\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprediction_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprediction_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpredicted_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "# Class: Children_palying\n",
    "\n",
    "filename = './children.wav' \n",
    "print_prediction(filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
