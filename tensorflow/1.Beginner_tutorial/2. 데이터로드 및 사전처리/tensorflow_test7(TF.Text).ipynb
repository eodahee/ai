{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 로드 및 사전처리\n",
    "## 7. TF.Text 사용법\n",
    "#### 참고사이트 : https://www.tensorflow.org/tutorials/tensorflow_text/intro?hl=ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unicode\n",
    "# 대부분 문자열은 UTF-8로 되어있는데 TF.Text의 ops를 통해 쉽게 변환 가능\n",
    "docs = tf.constant([u'Everything not saved will be lost.'.encode('UTF-16-BE'), u'Sad☹'.encode('UTF-16-BE')])\n",
    "utf8_docs = tf.strings.unicode_transcode(docs, input_encoding='UTF-16-BE', output_encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: (1,), types: tf.string>\n",
      "tf.Tensor([b'Everything not saved will be lost.' b'Sad\\xe2\\x98\\xb9'], shape=(2,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(docs)\n",
    "print(utf8_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kismi/.local/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py:180: batch_gather (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2017-10-25.\n",
      "Instructions for updating:\n",
      "`tf.batch_gather` is deprecated, please use `tf.gather` with `batch_dims=-1` instead.\n",
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost.'], [b'Sad\\xe2\\x98\\xb9']]\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "# 문자열을 token(단어, 숫자, /, 콤마, 온점 등)으로 분리하는 프로세스\n",
    "# tokenize는 비정형 tensor인 RaggedTensor를 반환\n",
    "\n",
    "# 공백문자의 토큰화(공백, 탭, 줄바꾸기)\n",
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n"
     ]
    }
   ],
   "source": [
    "# unicode script 토큰화\n",
    "# 공백(whitespace) 문자와 구두점 등을 분리(test6에서 했던 것에 비해 훨씬 간단)\n",
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "tokens = tokenizer.tokenize(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'\\xe4\\xbb\\x85', b'\\xe4\\xbb\\x8a', b'\\xe5\\xb9\\xb4', b'\\xe5\\x89\\x8d']]\n"
     ]
    }
   ],
   "source": [
    "# unicode split\n",
    "tokens = tf.strings.unicode_split([u\"仅今年前\".encode('UTF-8')], 'UTF-8')\n",
    "print(tokens.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'everything', b'not', b'saved', b'will', b'be', b'lost', b'.'], [b'Sad', b'\\xe2\\x98\\xb9']]\n",
      "[[0, 11, 15, 21, 26, 29, 33], [0, 3]]\n",
      "[[10, 14, 20, 25, 28, 33, 34], [3, 6]]\n"
     ]
    }
   ],
   "source": [
    "# offset\n",
    "# 문자열을 토큰화 할 때 토큰이 시작된 위치를 알아야 하는 경우가 종종 있음\n",
    "# tokenize_with_offsets() 함수를 통해 가능\n",
    "# starts와 limits로 offset 시작점, 끝나는점을 알 수 있음\n",
    "tokenizer = text.UnicodeScriptTokenizer()\n",
    "(tokens, offset_starts, offset_limits) = tokenizer.tokenize_with_offsets(['everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "print(tokens.to_list())\n",
    "print(offset_starts.to_list())\n",
    "print(offset_limits.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'Never', b'tell', b'me', b'the', b'odds.']]\n",
      "[[b\"It's\", b'a', b'trap!']]\n"
     ]
    }
   ],
   "source": [
    "# tf.data API의 예시\n",
    "docs = tf.data.Dataset.from_tensor_slices([['Never tell me the odds.'], [\"It's a trap!\"]])\n",
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokenized_docs = docs.map(lambda x: tokenizer.tokenize(x))\n",
    "iterator = iter(tokenized_docs)\n",
    "print(next(iterator).to_list())\n",
    "print(next(iterator).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[True, False, False, False, False, False], [True]]\n",
      "[[False, False, False, False, False, False], [False]]\n",
      "[[False, False, False, False, False, True], [True]]\n",
      "[[False, False, False, False, False, False], [False]]\n"
     ]
    }
   ],
   "source": [
    "# 다른 TF.Text의 기능\n",
    "# 유용한 전처리 작업을 패키지로 제공\n",
    "\n",
    "# Wordshape\n",
    "# 자연어 처리 모델에서 사용되는 일반적인 기능은 텍스트 문자열에 특정 속성이 있는지 확인하는 것\n",
    "# ex) 대문자 확인, 문장 부호 확인 등\n",
    "# wordshape는 입력 텍스트에서 다양한 패턴 일치를 위해 정규식 기반 함수로 도움\n",
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "\n",
    "f1 = text.wordshape(tokens, text.WordShape.HAS_TITLE_CASE) # Is capitalized?(대문자로 시작하는지)\n",
    "f2 = text.wordshape(tokens, text.WordShape.IS_UPPERCASE) # Are all letters uppercased?(대문자인지)\n",
    "f3 = text.wordshape(tokens, text.WordShape.HAS_SOME_PUNCT_OR_SYMBOL) # Does the token contain punctuation?(구둣점(.)인지)\n",
    "f4 = text.wordshape(tokens, text.WordShape.IS_NUMERIC_VALUE) # Is the token a number?(숫자인지)\n",
    "\n",
    "print(f1.to_list())\n",
    "print(f2.to_list())\n",
    "print(f3.to_list())\n",
    "print(f4.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'Everything not', b'not saved', b'saved will', b'will be', b'be lost.'], []]\n"
     ]
    }
   ],
   "source": [
    "# N-grams & Sliding Window\n",
    "# 크기가 n인 연속단어\n",
    "# 주로 Reduction.STRING_SUM과 Reduction.STRING_MEAN도 많이 쓰임\n",
    "tokenizer = text.WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(['Everything not saved will be lost.', u'Sad☹'.encode('UTF-8')])\n",
    "\n",
    "# Ngrams, in this case bi-gram (n = 2)\n",
    "bigrams = text.ngrams(tokens, 2, reduction_type=text.Reduction.STRING_JOIN)\n",
    "\n",
    "print(bigrams.to_list())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
